-----------------------------------------------------------------------------------------------------------------------
      name:  <unnamed>
       log:  /Users/kahrens/MyProjects/DMLGuide.github.io/assets/logs/example_GN2021.txt
  log type:  text
 opened on:  23 Mar 2025, 12:21:46

. 
. use "dta/GN2021", clear

. 
. // shorten the name of the causal variable of interest to sd_EE
. rename sd_of_gen_mean_temp_anom_EE sd_EE

. 
. // replicate results with no controls in Table 1, column (3).
. eststo m0: ///
> reg A198new sd_EE, robust

Linear regression                               Number of obs     =         75
                                                F(1, 73)          =      13.54
                                                Prob > F          =     0.0004
                                                R-squared         =     0.1478
                                                Root MSE          =     .51072

------------------------------------------------------------------------------
             |               Robust
     A198new | Coefficient  std. err.      t    P>|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
       sd_EE |  -1.923264   .5227138    -3.68   0.000    -2.965031   -.8814967
       _cons |   5.006289    .139419    35.91   0.000     4.728428    5.284151
------------------------------------------------------------------------------

. 
. // replicate Figure 5
. reg A198new sd_EE, robust

Linear regression                               Number of obs     =         75
                                                F(1, 73)          =      13.54
                                                Prob > F          =     0.0004
                                                R-squared         =     0.1478
                                                Root MSE          =     .51072

------------------------------------------------------------------------------
             |               Robust
     A198new | Coefficient  std. err.      t    P>|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
       sd_EE |  -1.923264   .5227138    -3.68   0.000    -2.965031   -.8814967
       _cons |   5.006289    .139419    35.91   0.000     4.728428    5.284151
------------------------------------------------------------------------------

. predict hat
(option xb assumed; fitted values)

. twoway (scatter A198new sd_EE, msymbol(o) mlabel(isocode))                      ///
>         (line hat sd_EE, sort lwidth(thick)),                                                   ///
>         legend(off) xlabel(0 0.25 0.5, nogrid)                                                  ///
>         ylabel(3 4 5 6, nogrid)                                                                                 ///
>         note("(coef = -1.92, t = -3.68)")                                                               ///
>         ytitle("Importance of tradition")                                                               ///
>         xtitle("Climatic instability")

. drop hat

. graph export "logs/GN_Figure5.png", replace
file /Users/kahrens/MyProjects/DMLGuide.github.io/assets/logs/GN_Figure5.png saved as PNG format

. 
. // replicate results with controls in Table 1, column (4).
. eststo m1: ///
> reg A198new sd_EE v104_ee settlement_ee polhierarchies_ee loggdp, robust

Linear regression                               Number of obs     =         74
                                                F(5, 68)          =       7.63
                                                Prob > F          =     0.0000
                                                R-squared         =     0.3876
                                                Root MSE          =      .4485

-----------------------------------------------------------------------------------
                  |               Robust
          A198new | Coefficient  std. err.      t    P>|t|     [95% conf. interval]
------------------+----------------------------------------------------------------
            sd_EE |  -1.823516   .6958312    -2.62   0.011    -3.212026   -.4350068
          v104_ee |   .0054164   .0053679     1.01   0.317    -.0052951    .0161279
    settlement_ee |  -.0651081   .0347562    -1.87   0.065     -.134463    .0042467
polhierarchies_ee |   .0127799   .0971405     0.13   0.896     -.181061    .2066208
           loggdp |  -.1648961   .0486385    -3.39   0.001    -.2619527   -.0678394
            _cons |   6.576112   .5359645    12.27   0.000     5.506611    7.645613
-----------------------------------------------------------------------------------

. 
. esttab m0 m1

--------------------------------------------
                      (1)             (2)   
                  A198new         A198new   
--------------------------------------------
sd_EE              -1.923***       -1.824*  
                  (-3.68)         (-2.62)   

v104_ee                           0.00542   
                                   (1.01)   

settlemen~ee                      -0.0651   
                                  (-1.87)   

polhierar~ee                       0.0128   
                                   (0.13)   

loggdp                             -0.165** 
                                  (-3.39)   

_cons               5.006***        6.576***
                  (35.91)         (12.27)   
--------------------------------------------
N                      75              74   
--------------------------------------------
t statistics in parentheses
* p<0.05, ** p<0.01, *** p<0.001

. estout m0 m1,                                                                                                        
>    ///
>         label modelwidth(15) collabels(none)                                                    ///
>         cells(b(fmt(3)) se(par fmt(3)))                                                                 ///
>         varlabels(_cons Constant)

----------------------------------------------------
                                  m0              m1
----------------------------------------------------
Climatic instabili~u          -1.923          -1.824
                             (0.523)         (0.696)
Distance from the ~e                           0.005
                                             (0.005)
Economic complexit~r                          -0.065
                                             (0.035)
Political hierarch~E                           0.013
                                             (0.097)
Log (per capita GDP)                          -0.165
                                             (0.049)
Constant                       5.006           6.576
                             (0.139)         (0.536)
----------------------------------------------------

. 
. 
. /*
> A useful robustness check is to ask whether the results with controls are sensitive to the assumption that the contro
> ls enter linearly.
> DML can help answer this question.
> Below we estimate the model with controls using DML and the partially-linear model (PLM).
> The specification is very simple to start: we keep the assumption that the controls enter separately, but drop linear
> ity.
> 
> The estimation procedure takes the following steps:
> 
> 1. Specify the DML model: in this case, the PLM.
> 
> 2. Specify the learners: choose the learners for the conditional expectation functions for the outcome variable and t
> he causal variable of interest.
> 
> 3. Cross-fitting: obtain cross-fit estimates of the two conditional expectations.
> 
> 4. Estimation: regress the residualized outcome variable on the residualized causal variable of interest.
> 
> We have no strong priors on whether a linear or non-linear learner is more appropriate, or whether regularization is 
> needed at all.
> Hence we use 3 learners in this example: unregularized OLS, cross-validated Lasso, and a random forest.
> We also use short-stacking to combine the estimated conditional expectations from these 3 learners.
> Short-stacking is computationally cheap, and for the same reason the initial estimation is done once, i.e., we do not
>  resample (re-cross-fit based on a different split).
> In the example below, short-stacking creates an ensemble learner with weights based on non-negative nonlinear least s
> quares (NNLS) where the weights are constrained to lie in the [0,1] interval and to sum to 1.
> 
> The dataset is small, and so we choose 10-fold cross-fitting.
> This means that learners are trained on about 66-67 observations, and OOS predictions are obtained for the remaining 
> 7-8 observations.
> (If the dataset were larger, we could e.g. use 5-fold cross-fitting and save some computation time.)
> 
> */
. 
. // In the G-N estimations, GDP is missing for one country.
. // For simplicity, we just drop this one country so that it is never used.
. drop if loggdp==.
(1 observation deleted)

. 
. // To make the code more readable, define macros Y, D and X:
. global Y A198new

. global D sd_EE

. global X v104_ee settlement_ee polhierarchies_ee loggdp

. 
. 
. // In Stata, we use the ddml package along with the pystacked package.
. // pystacked is a front-end to sklearn's stacking regression.
. which ddml
/Users/kahrens/Library/Application Support/Stata/ado/plus/d/ddml.ado
*! ddml v1.4.4
*! last edited: 30aug2024
*! authors: aa/ms

. which pystacked
/Users/kahrens/Library/Application Support/Stata/ado/plus/p/pystacked.ado
*! pystacked v0.7.6
*! last edited: 3jan2025
*! authors: aa/ms

. 
. // Step 1: Specify the model.
. // We use the partially-linear model with 10-fold cross-fitting and a single cross-fit split.
. ddml init partial, kfolds(10) reps(1)

. 
. // Step 2: Choose the learners.
. // We use unregularized OLS, cross-validated lasso and random forest.
. // We use the same learners for Y and X for expositional simplicity only.
. ddml E[Y|X]: pystacked $Y $X                                                            ///
>                                                                 || method(ols)                          ///
>                                                                 || method(lassocv)                      ///
>                                                                 || method(rf)                           ///
>                                                                 , type(reg)
Learner Y1_pystacked added successfully.

. 
. ddml E[D|X]: pystacked $D $X                                                            ///
>                                                                 || method(ols)                          ///
>                                                                 || method(lassocv)                      ///
>                                                                 || method(rf)                           ///
>                                                                 , type(reg)
Learner D1_pystacked added successfully.

. 
. // Check that the learners have been entered correctly.
. ddml desc, learners

Model:                  partial, crossfit folds k=10, resamples r=1
Mata global (mname):    m0
Dependent variable (Y): A198new
 A198new learners:      Y1_pystacked
D equations (1):        sd_EE
 sd_EE learners:        D1_pystacked

Y learners (detail):
 Learner:     Y1_pystacked
              est cmd: pystacked A198new v104_ee settlement_ee polhierarchies_ee loggdp || method(ols) || method(lassoc
> v) || method(rf) , type(reg)
D learners (detail):
 Learner:     D1_pystacked
              est cmd: pystacked sd_EE v104_ee settlement_ee polhierarchies_ee loggdp || method(ols) || method(lassocv)
>  || method(rf) , type(reg)

. 
. // Step 3: Cross-fit.
. // We use short-stacking to combine the conditional expectations
. // of the different learners.
. // The default behavior of ddml+pystacked is to do standard stacking,
. // which is computationally intensive; short-stacking is faster.
. // To suppress standard stacking, we use the nostdstack option.
. ddml crossfit, shortstack nostdstack
Cross-fitting E[y|X] equation: A198new
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking
Cross-fitting E[D|X] equation: sd_EE
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking

. 
. // Step 4: Estimation
. // After Step 3, we have 4 sets of residualized Y and 4 of residualized D,
. // based on 4 sets of estimated conditional expectations:
. // one for each of the 3 learners plus one stacked (ensemble) estimate.
. // By default, ddml reports the short-stacked results:
. // the residualized dependent variable Y is regressed on the residualized
. // causal variable of interest D using OLS.
. // Note that if you execute the code your your results will vary
. // (usually only slightly) because the seed hasn't been set.
. ddml estimate, robust


Model:                  partial, crossfit folds k=10, resamples r=1
Mata global (mname):    m0
Dependent variable (Y): A198new
 A198new learners:      Y1_pystacked
D equations (1):        sd_EE
 sd_EE learners:        D1_pystacked

DDML estimation results:
spec  r     Y learner     D learner         b        SE 
  ss  1  [shortstack]          [ss]    -2.164    (0.741)

Shortstack DDML model
y-E[y|X]  = y-Y_A198new_ss_1                       Number of obs   =        74
D-E[D|X]  = D-D_sd_EE_ss_1 
------------------------------------------------------------------------------
             |               Robust
     A198new | Coefficient  std. err.      z    P>|z|     [95% conf. interval]
-------------+----------------------------------------------------------------
       sd_EE |   -2.16408   .7413585    -2.92   0.004    -3.617116    -.711044
       _cons |   .0000172    .052501     0.00   1.000     -.102883    .1029173
------------------------------------------------------------------------------
Stacking final estimator: nnls1


. 
. /*
> We haven't set the random number seed, so the results will vary based on the randomization of the cross-fit split and
>  any randomization used by the learners.
> But typically the DML short-stacked results are quite close to the original linear specification use by G-N.
> A natural interpretation is that the G-N results still stand in this robustness check.
> 
> This does not mean, however, that the linear specification was the "right" choice.
> Depending on the randomization in the cross-fit split and the learners,
> the short-stacking weights will typically put a low weight on unregularized OLS.
> The random forest learner will also typically get a substantial weight in both of the conditional expectation estimat
> es.
> This suggests that some nonlinearity is present, but that the assumption of linearity does not substantially affect t
> he results.
> */
. 
. // Display the short-stack weights.
. ddml extract, show(ssweights)

short-stacked weights across resamples for A198new
final stacking estimator: nnls1
             learner  mean_weight        rep_1
    ols            1    2.176e-19    2.176e-19
lassocv            2    .48057557    .48057557
     rf            3    .51942443    .51942443

short-stacked weights across resamples for sd_EE
final stacking estimator: nnls1
             learner  mean_weight        rep_1
    ols            1            0            0
lassocv            2    .41567395    .41567395
     rf            3    .58432605    .58432605

. 
. /*
> The residualized Y and D can also be inspected and used in other specifications.
> The G-N dataset is very small and we can plot the results as in Figure 5 of the original paper.
> Below we look at scatterplots where the same learner is used for both Y and D.
> But we could also look at combinations, e.g., unregularized OLS for Y and RF for D.
> This is related to a version of stacking sometimes called "single-best", where instead of deriving weights using NNLS
> , all the weight is put on the learner with the best OOS performance (smallest MSPE).
> */
. 
. // ddml creates conditional expectations for each learner.
. // With pystacked, the learners are numbered.
. // The final "_1" identifies the resample (the single cross-fit split).
. // Create residualized Y and D for each learner.
. cap drop Y_L1_r Y_L2_r Y_L3_r Y_ss_r D_L1_r D_L2_r D_L3_r D_ss_r

. gen Y_L1_r = $Y-Y1_pystacked_L1_1

. gen Y_L2_r = $Y-Y1_pystacked_L2_1

. gen Y_L3_r = $Y-Y1_pystacked_L3_1

. gen Y_ss_r = $Y-Y_A198new_ss_1

. gen D_L1_r = $D-D1_pystacked_L1_1

. gen D_L2_r = $D-D1_pystacked_L2_1

. gen D_L3_r = $D-D1_pystacked_L3_1

. gen D_ss_r = $D-D_sd_EE_ss_1

. 
. // 4 specification and scatterplots
. // Y and D learners = OLS
. // We suppress the output for the individual OLS regressions to save space.
. qui reg Y_L1_r D_L1_r, robust

. est store ddml_L1_L1, title("OLS learners")

. cap drop hat_1_1

. predict hat_1_1
(option xb assumed; fitted values)

. twoway (scatter Y_L1_r D_L1_r, msymbol(o) mlabel(isocode))                      ///
>         (line hat_1_1 D_L1_r, sort lwidth(thick)),                                              ///
>         legend(off)                                                                                                  
>            ///
>         xlabel(-0.3 -0.2 -0.1 0 0.1 0.2 0.3, nogrid)                                    ///
>         ylabel(-2 -1 0 1 2, nogrid)                                                                             ///
>         title("Y and D: Unregularized OLS")                                                             ///
>         ytitle("Importance of tradition")                                                               ///
>         xtitle("Climatic instability")                                                                  ///
>         name(OLS, replace)

. // Y and D learners = CV Lasso
. qui reg Y_L2_r D_L2_r, robust

. est store ddml_L2_L2, title("Lasso learners")

. cap drop hat_2_2

. predict hat_2_2
(option xb assumed; fitted values)

. twoway (scatter Y_L2_r D_L2_r, msymbol(o) mlabel(isocode))                      ///
>         (line hat_2_2 D_L2_r, sort lwidth(thick)),                                              ///
>         legend(off)                                                                                                  
>            ///
>         xlabel(-0.3 -0.2 -0.1 0 0.1 0.2 0.3, nogrid)                                    ///
>         ylabel(-2 -1 0 1 2, nogrid)                                                                             ///
>         title("Y and D: Lasso")                                                                                 ///
>         ytitle("Importance of tradition")                                                               ///
>         xtitle("Climatic instability")                                                                  ///
>         name(Lasso, replace)

. // Y and D learners = Random Forest
. qui reg Y_L3_r D_L3_r, robust

. est store ddml_L3_L3, title("RF learners")

. cap drop hat_3_3

. predict hat_3_3
(option xb assumed; fitted values)

. twoway (scatter Y_L3_r D_L3_r, msymbol(o) mlabel(isocode))                      ///
>         (line hat_3_3 D_L3_r, sort lwidth(thick)),                                              ///
>         legend(off)                                                                                                  
>            ///
>         xlabel(-0.3 -0.2 -0.1 0 0.1 0.2 0.3, nogrid)                                    ///
>         ylabel(-2 -1 0 1 2, nogrid)                                                                             ///
>         title("Y and D: Random Forest")                                                                 ///
>         ytitle("Importance of tradition")                                                               ///
>         xtitle("Climatic instability")                                                                  ///
>         name(RF, replace)

. // Y and D learners = Short-stacked (ensemble of OLS, Lasso, RF)
. qui reg Y_ss_r D_ss_r, robust

. est store ddml_SS_SS, title("SS learners")

. cap drop hat_ss_ss

. predict hat_ss_ss
(option xb assumed; fitted values)

. twoway (scatter Y_ss_r D_ss_r, msymbol(o) mlabel(isocode))                      ///
>         (line hat_ss_ss D_ss_r, sort lwidth(thick)),                                    ///
>         legend(off)                                                                                                  
>            ///
>         xlabel(-0.3 -0.2 -0.1 0 0.1 0.2 0.3, nogrid)                                    ///
>         ylabel(-2 -1 0 1 2, nogrid)                                                                             ///
>         title("Y and D: Short-stacked")                                                                 ///
>         ytitle("Importance of tradition")                                                               ///
>         xtitle("Climatic instability")                                                                  ///
>         name(SS, replace)

. 
. // Compare DML estimations for learner combinations:
. // nb: Code uses estout by Ben Jann; install from SSC Archives.
. estout ddml_L1_L1 ddml_L2_L2 ddml_L3_L3 ddml_SS_SS,                                     ///
>         label modelwidth(15) collabels(none)                                                    ///
>         rename (D_L1_r D_r D_L2_r D_r D_L3_r D_r D_ss_r D_r)                    ///
>         cells(b(fmt(3)) se(par fmt(3)))                                                                 ///
>         varlabels(_cons Constant)

------------------------------------------------------------------------------------
                        OLS learners  Lasso learners     RF learners     SS learners
------------------------------------------------------------------------------------
D_r                           -1.799          -1.839          -2.147          -2.164
                             (0.665)         (0.668)         (0.680)         (0.741)
Constant                      -0.020          -0.011           0.010           0.000
                             (0.055)         (0.055)         (0.054)         (0.053)
------------------------------------------------------------------------------------

. 
. // Compare via scatterplot:
. graph combine OLS Lasso RF SS

. graph export "images/GN_4learners.png", replace
file /Users/kahrens/MyProjects/DMLGuide.github.io/assets/images/GN_4learners.png saved as PNG format

. 
. /*
> The procedure above is suitable for "work-in-progress".
> For "final" results (e.g., for publication), a researcher should:
> 
> 1. Set the random number seed(s) for replicability.
> 2. Consider using multiple cross-fit splits and aggregate them.
> 3. Consider using other stacking methods.
> 
> Cross-fitting introduces randomness by using a random split into cross-fit folds.
> A straightfoward way to reduce the sensitivity of the results to the split is to re-estimate using different cross-fi
> t splits and aggregate the results.
> Aggregation can use the median or the mean.
> 
> Short-stacking stacking is computationally appealing but there are other options.
> Standard stacking - stacking separately for each cross-fit estimation - is also a possibility.
> "Pooled stacking" is similar to standard stacking except that the weights for combining learners are based on the OOS
>  predictions for the entire sample (rather than for each cross-fit fold separately).
> 
> The example below illustrates.
> */
. 
. 
. // "Final" results:
. // 1. Set seed for replicability.
. // 2. Report all 3 stacking methods.
. // 3. 11 separate cross-fit splits and median aggregation.
. 
. // The choice of 11 separate cross-fit splits is so that the median is well-defined.
. 
. // Set the seed.
. set seed 42

. 
. // Step 1: Specify the model.
. // Use 11 separate resample splits.
. ddml init partial, kfolds(10) reps(11)
warning - model m0 already exists
all existing model results and variables will
be dropped and model m0 will be re-initialized

. 
. // Step 2: Choose the learners.
. // Conditional expections are defined as before.
. ddml E[Y|X]: pystacked $Y $X                                                            ///
>                                                                 || method(ols)                          ///
>                                                                 || method(lassocv)                      ///
>                                                                 || method(rf)                           ///
>                                                                 , type(reg)
Learner Y1_pystacked added successfully.

. 
. ddml E[D|X]: pystacked $D $X                                                            ///
>                                                                 || method(ols)                          ///
>                                                                 || method(lassocv)                      ///
>                                                                 || method(rf)                           ///
>                                                                 , type(reg)
Learner D1_pystacked added successfully.

. 
. // Step 3: Cross-fit.
. // Standard stacking is the default behavior of ddml+pystacked.
. // Pooled-stacking is requested separately.
. ddml crossfit, shortstack poolstack
Cross-fitting E[y|X] equation: A198new
Resample 1...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking...completed pooled-stacki
> ng
Resample 2...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking...completed pooled-stacki
> ng
Resample 3...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking...completed pooled-stacki
> ng
Resample 4...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking...completed pooled-stacki
> ng
Resample 5...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking...completed pooled-stacki
> ng
Resample 6...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking...completed pooled-stacki
> ng
Resample 7...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking...completed pooled-stacki
> ng
Resample 8...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking...completed pooled-stacki
> ng
Resample 9...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking...completed pooled-stacki
> ng
Resample 10...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking...completed pooled-stacki
> ng
Resample 11...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking...completed pooled-stacki
> ng
Cross-fitting E[D|X] equation: sd_EE
Resample 1...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking...completed pooled-stacki
> ng
Resample 2...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking...completed pooled-stacki
> ng
Resample 3...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking...completed pooled-stacki
> ng
Resample 4...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking...completed pooled-stacki
> ng
Resample 5...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking...completed pooled-stacki
> ng
Resample 6...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking...completed pooled-stacki
> ng
Resample 7...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking...completed pooled-stacki
> ng
Resample 8...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking...completed pooled-stacki
> ng
Resample 9...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking...completed pooled-stacki
> ng
Resample 10...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking...completed pooled-stacki
> ng
Resample 11...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking...completed pooled-stacki
> ng

. 
. // Step 4: Estimation.
. // Median-aggregated short-stacked results are reported by default.
. // We also ask for the standard and pooled-stacking results.
. ddml estimate, robust


Model:                  partial, crossfit folds k=10, resamples r=11
Mata global (mname):    m0
Dependent variable (Y): A198new
 A198new learners:      Y1_pystacked
D equations (1):        sd_EE
 sd_EE learners:        D1_pystacked

DDML estimation results:
spec  r     Y learner     D learner         b        SE 
  st  1  Y1_pystacked  D1_pystacked    -2.160    (0.784)
  ss  1  [shortstack]          [ss]    -2.326    (0.802)
  ps  1   [poolstack]          [ps]    -2.366    (0.801)
  st  2  Y1_pystacked  D1_pystacked    -1.801    (0.771)
  ss  2  [shortstack]          [ss]    -1.907    (0.769)
  ps  2   [poolstack]          [ps]    -1.865    (0.780)
  st  3  Y1_pystacked  D1_pystacked    -2.150    (0.737)
  ss  3  [shortstack]          [ss]    -2.141    (0.723)
  ps  3   [poolstack]          [ps]    -2.190    (0.729)
  st  4  Y1_pystacked  D1_pystacked    -2.257    (0.808)
  ss  4  [shortstack]          [ss]    -2.058    (0.790)
  ps  4   [poolstack]          [ps]    -2.094    (0.808)
  st  5  Y1_pystacked  D1_pystacked    -2.269    (0.796)
  ss  5  [shortstack]          [ss]    -2.265    (0.797)
  ps  5   [poolstack]          [ps]    -2.268    (0.794)
  st  6  Y1_pystacked  D1_pystacked    -2.272    (0.762)
  ss  6  [shortstack]          [ss]    -2.108    (0.764)
  ps  6   [poolstack]          [ps]    -2.142    (0.749)
  st  7  Y1_pystacked  D1_pystacked    -2.269    (0.734)
  ss  7  [shortstack]          [ss]    -2.252    (0.735)
  ps  7   [poolstack]          [ps]    -2.255    (0.726)
  st  8  Y1_pystacked  D1_pystacked    -1.912    (0.715)
  ss  8  [shortstack]          [ss]    -1.892    (0.733)
  ps  8   [poolstack]          [ps]    -1.940    (0.725)
  st  9  Y1_pystacked  D1_pystacked    -2.077    (0.758)
  ss  9  [shortstack]          [ss]    -1.941    (0.749)
  ps  9   [poolstack]          [ps]    -2.030    (0.752)
  st 10  Y1_pystacked  D1_pystacked    -2.300    (0.714)
  ss 10  [shortstack]          [ss]    -2.231    (0.735)
  ps 10   [poolstack]          [ps]    -2.240    (0.741)
  st 11  Y1_pystacked  D1_pystacked    -1.702    (0.783)
  ss 11  [shortstack]          [ss]    -1.911    (0.763)
  ps 11   [poolstack]          [ps]    -1.951    (0.773)

Mean/med    Y learner     D learner         b        SE 
  st mn  Y1_pystacked  D1_pystacked    -2.106    (0.782)
  ss mn  [shortstack]          [ss]    -2.094    (0.774)
  ps mn   [poolstack]          [ps]    -2.122    (0.774)
  st md  Y1_pystacked  D1_pystacked    -2.160    (0.770)
  ss md  [shortstack]          [ss]    -2.108    (0.767)
  ps md   [poolstack]          [ps]    -2.142    (0.760)

Shortstack DDML model (median over 11 resamples)
y-E[y|X]  = y-Y_A198new_ss                         Number of obs   =        74
D-E[D|X]  = D-D_sd_EE_ss 
------------------------------------------------------------------------------
             |               Robust
     A198new | Coefficient  std. err.      z    P>|z|     [95% conf. interval]
-------------+----------------------------------------------------------------
       sd_EE |  -2.108401   .7674987    -2.75   0.006    -3.612671   -.6041313
------------------------------------------------------------------------------
Stacking final estimator: nnls1

Summary over 11 resamples:
       D eqn      mean       min       p25       p50       p75       max
       sd_EE     -2.0937   -2.3256   -2.2521   -2.1084   -1.9105   -1.8915

. ddml estimate, spec(st) rep(md) notable replay

Median over 11 stacking resamples
y-E[y|X]  = y-Y1_pystacked                         Number of obs   =        74
D-E[D|X]  = D-D1_pystacked 
------------------------------------------------------------------------------
             |               Robust
     A198new | Coefficient  std. err.      z    P>|z|     [95% conf. interval]
-------------+----------------------------------------------------------------
       sd_EE |  -2.160379   .7700377    -2.81   0.005    -3.669625   -.6511328
------------------------------------------------------------------------------
Stacking final estimator: nnls1

Summary over 11 resamples:
       D eqn      mean       min       p25       p50       p75       max
       sd_EE     -2.1062   -2.2996   -2.2694   -2.1604   -1.9118   -1.7016

. ddml estimate, spec(ps) rep(md) notable replay

Poolstack DDML model (median over 11 resamples)
y-E[y|X]  = y-Y_A198new_ps                         Number of obs   =        74
D-E[D|X]  = D-D_sd_EE_ps 
------------------------------------------------------------------------------
             |               Robust
     A198new | Coefficient  std. err.      z    P>|z|     [95% conf. interval]
-------------+----------------------------------------------------------------
       sd_EE |  -2.142118   .7601912    -2.82   0.005    -3.632065   -.6521705
------------------------------------------------------------------------------
Stacking final estimator: nnls1

Summary over 11 resamples:
       D eqn      mean       min       p25       p50       p75       max
       sd_EE     -2.1219   -2.3657   -2.2550   -2.1421   -1.9505   -1.8655

. 
. /*
> The median results from the 11 DML estimations are again similar to the original G-N results.
> The conclusion is again that their specification is robust to dropping the linearity assumption.
> 
> Note that the variation across the 11 refits introduced by the randomization of the cross-fit split is not trivial - 
> the DML coefficient estimates range from about -1.7 to about -2.3 - but not enough to overturn the conclusion above.
> Increasing the number of refits is worth considering here.
> 
> It's also still the case that there is evidence of nonlinearity:
> all 3 stacking procedures tend to put a low weight on unregularized OLS.
> Now the nonlinear random forest learner typically gets the biggest weight.
> Again, this is evidence of some nonlinearity, but not enough to overturn the OLS-based results.
> 
> */
. 
. 
. // Display the stacking weights.
. // As before, unregularized OLS gets a low weight for both Y and D,
. // and the random forest learner gets a substantial weight in both.
. ddml extract, show(weights)

mean stacking weights across folds/resamples for Y1_pystacked (A198new)
final stacking estimator: nnls1
             learner  mean_weight        rep_1        rep_2        rep_3        rep_4        rep_5        rep_6
    ols            1    .20944904    .18951668    .09985336    .19016805    .19588711    .28615956    .26567238
lassocv            2    .35366105    .35626782    .36574747    .44812183     .3487006    .30431179    .26965783
     rf            3    .43688991    .45421551    .53439917    .36171013    .45541228    .40952865    .46466979

               rep_7        rep_8        rep_9       rep_10       rep_11
    ols    .23126797    .19322046    .17267556    .32139138    .15812698
lassocv    .36309904    .39886426     .3880916    .25554098    .39186835
     rf      .405633    .40791528    .43923285    .42306764    .45000467

mean stacking weights across folds/resamples for D1_pystacked (sd_EE)
final stacking estimator: nnls1
             learner  mean_weight        rep_1        rep_2        rep_3        rep_4        rep_5        rep_6
    ols            1    .00672472    .02516423    .00982363    1.756e-18    .00645771    5.504e-19    .00434356
lassocv            2    .32615507    .35428628    .30116974    .35093254    .30099416    .33042824    .30649569
     rf            3    .66712021    .62054949    .68900665    .64906746    .69254812    .66957176    .68916075

               rep_7        rep_8        rep_9       rep_10       rep_11
    ols    3.554e-18    3.114e-18    .01617726    2.332e-18    .01200552
lassocv    .34408303    .32774874    .26841759    .35133844    .35181134
     rf    .65591697    .67225126    .71540516    .64866156    .63618314

short-stacked weights across resamples for A198new
final stacking estimator: nnls1
             learner  mean_weight        rep_1        rep_2        rep_3        rep_4        rep_5        rep_6
    ols            1     .2780639    .25021548    .44603695    .15314268    .68821837            0    8.412e-18
lassocv            2    .15544152    1.031e-18    .11247891     .3152944    1.086e-19    .49763273    .31023968
     rf            3    .56649459    .74978452    .44148413    .53156292    .31178163    .50236727    .68976032

               rep_7        rep_8        rep_9       rep_10       rep_11
    ols    .11566583    .08583231    .34503221    .49644733    .47811173
lassocv     .1796082    .28357956            0    1.952e-18    .01102318
     rf    .70472597    .63058813    .65496779    .50355267    .51086509

short-stacked weights across resamples for sd_EE
final stacking estimator: nnls1
             learner  mean_weight        rep_1        rep_2        rep_3        rep_4        rep_5        rep_6
    ols            1    1.490e-18            0            0    3.209e-18            0    6.072e-18    3.326e-18
lassocv            2    .33170229    .37136168    .34520346    .32768056    .27627495     .3283087    .38245886
     rf            3    .66829771    .62863832    .65479654    .67231944    .72372505     .6716913    .61754114

               rep_7        rep_8        rep_9       rep_10       rep_11
    ols    1.502e-18            0            0    2.277e-18            0
lassocv    .40774365    .37568256    .26729688    .30843848    .25827545
     rf    .59225635    .62431744    .73270312    .69156152    .74172455

pool-stacked weights across resamples for A198new
final stacking estimator: nnls1
             learner  mean_weight        rep_1        rep_2        rep_3        rep_4        rep_5        rep_6
    ols            1    .08811846            0    7.589e-19    .04138413     .0294995    .07251657    .35507842
lassocv            2    .45351487    .50761675    .45883061    .55971403    .49087864    .49668767    .15552255
     rf            3    .45836667    .49238325    .54116939    .39890184    .47962187    .43079576    .48939903

               rep_7        rep_8        rep_9       rep_10       rep_11
    ols    .18986704            0    .08693407     .1940233    4.032e-18
lassocv    .38420608    .57463305    .44870082    .38016296    .53171039
     rf    .42592688    .42536695    .46436511    .42581374    .46828961

pool-stacked weights across resamples for sd_EE
final stacking estimator: nnls1
             learner  mean_weight        rep_1        rep_2        rep_3        rep_4        rep_5        rep_6
    ols            1    5.284e-18            0            0            0            0    5.974e-18    5.855e-18
lassocv            2    .33309216     .3762113    .31351867    .35642406    .30378299    .32386615    .31441792
     rf            3    .66690784     .6237887    .68648133    .64357594    .69621701    .67613385    .68558208

               rep_7        rep_8        rep_9       rep_10       rep_11
    ols            0    2.011e-17    2.618e-17            0            0
lassocv    .34445217    .32863626    .28624329    .35260336    .36385761
     rf    .65554783    .67136374    .71375671    .64739664    .63614239

. 
. ********************************************************************************
. 
. cap log close
