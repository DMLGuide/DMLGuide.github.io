----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
      name:  <unnamed>
       log:  /Users/kahrens/MyProjects/DMLGuide.github.io/assets/logs/example_GN2021.txt
  log type:  text
 opened on:  25 Mar 2025, 13:41:28

. 
. use "https://dmlguide.github.io/assets/dta/GN2021.dta", clear

. 
. // shorten the name of the causal variable of interest to sd_EE
. rename sd_of_gen_mean_temp_anom_EE sd_EE

. 
. // replicate results with no controls in Table 1, column (3).
. eststo m0: ///
> reg A198new sd_EE, robust

Linear regression                               Number of obs     =         75
                                                F(1, 73)          =      13.54
                                                Prob > F          =     0.0004
                                                R-squared         =     0.1478
                                                Root MSE          =     .51072

------------------------------------------------------------------------------
             |               Robust
     A198new | Coefficient  std. err.      t    P>|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
       sd_EE |  -1.923264   .5227138    -3.68   0.000    -2.965031   -.8814967
       _cons |   5.006289    .139419    35.91   0.000     4.728428    5.284151
------------------------------------------------------------------------------

. 
. // replicate Figure 5
. reg A198new sd_EE, robust

Linear regression                               Number of obs     =         75
                                                F(1, 73)          =      13.54
                                                Prob > F          =     0.0004
                                                R-squared         =     0.1478
                                                Root MSE          =     .51072

------------------------------------------------------------------------------
             |               Robust
     A198new | Coefficient  std. err.      t    P>|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
       sd_EE |  -1.923264   .5227138    -3.68   0.000    -2.965031   -.8814967
       _cons |   5.006289    .139419    35.91   0.000     4.728428    5.284151
------------------------------------------------------------------------------

. predict hat
(option xb assumed; fitted values)

. twoway (scatter A198new sd_EE, msymbol(o) mlabel(isocode))                      ///
>         (line hat sd_EE, sort lwidth(thick)),                                                   ///
>         legend(off) xlabel(0 0.25 0.5, nogrid)                                                  ///
>         ylabel(3 4 5 6, nogrid)                                                                                 ///
>         note("(coef = -1.92, t = -3.68)")                                                               ///
>         ytitle("Importance of tradition")                                                               ///
>         xtitle("Climatic instability")

. drop hat

. graph export "logs/GN_Figure5.png", replace
file /Users/kahrens/MyProjects/DMLGuide.github.io/assets/logs/GN_Figure5.png saved as PNG format

. 
. // replicate results with controls in Table 1, column (4).
. eststo m1: ///
> reg A198new sd_EE v104_ee settlement_ee polhierarchies_ee loggdp, robust

Linear regression                               Number of obs     =         74
                                                F(5, 68)          =       7.63
                                                Prob > F          =     0.0000
                                                R-squared         =     0.3876
                                                Root MSE          =      .4485

-----------------------------------------------------------------------------------
                  |               Robust
          A198new | Coefficient  std. err.      t    P>|t|     [95% conf. interval]
------------------+----------------------------------------------------------------
            sd_EE |  -1.823516   .6958312    -2.62   0.011    -3.212026   -.4350068
          v104_ee |   .0054164   .0053679     1.01   0.317    -.0052951    .0161279
    settlement_ee |  -.0651081   .0347562    -1.87   0.065     -.134463    .0042467
polhierarchies_ee |   .0127799   .0971405     0.13   0.896     -.181061    .2066208
           loggdp |  -.1648961   .0486385    -3.39   0.001    -.2619527   -.0678394
            _cons |   6.576112   .5359645    12.27   0.000     5.506611    7.645613
-----------------------------------------------------------------------------------

. 
. esttab m0 m1

--------------------------------------------
                      (1)             (2)   
                  A198new         A198new   
--------------------------------------------
sd_EE              -1.923***       -1.824*  
                  (-3.68)         (-2.62)   

v104_ee                           0.00542   
                                   (1.01)   

settlemen~ee                      -0.0651   
                                  (-1.87)   

polhierar~ee                       0.0128   
                                   (0.13)   

loggdp                             -0.165** 
                                  (-3.39)   

_cons               5.006***        6.576***
                  (35.91)         (12.27)   
--------------------------------------------
N                      75              74   
--------------------------------------------
t statistics in parentheses
* p<0.05, ** p<0.01, *** p<0.001

. estout m0 m1,                                                                                                           ///
>         label modelwidth(15) collabels(none)                                                    ///
>         cells(b(fmt(3)) se(par fmt(3)))                                                                 ///
>         varlabels(_cons Constant)

----------------------------------------------------
                                  m0              m1
----------------------------------------------------
Climatic instabili~u          -1.923          -1.824
                             (0.523)         (0.696)
Distance from the ~e                           0.005
                                             (0.005)
Economic complexit~r                          -0.065
                                             (0.035)
Political hierarch~E                           0.013
                                             (0.097)
Log (per capita GDP)                          -0.165
                                             (0.049)
Constant                       5.006           6.576
                             (0.139)         (0.536)
----------------------------------------------------

. 
. // In the G-N estimations, GDP is missing for one country.
. // For simplicity, we just drop this one country so that it is never used.
. drop if loggdp==.
(1 observation deleted)

. 
. // To make the code more readable, define macros Y, D and X:
. global Y A198new

. global D sd_EE

. global X v104_ee settlement_ee polhierarchies_ee loggdp

. 
. 
. // In Stata, we use the ddml package along with the pystacked package.
. // pystacked is a front-end to sklearn's stacking regression.
. which ddml
/Users/kahrens/Library/Application Support/Stata/ado/plus/d/ddml.ado
*! ddml v1.4.4
*! last edited: 30aug2024
*! authors: aa/ms

. which pystacked
/Users/kahrens/Library/Application Support/Stata/ado/plus/p/pystacked.ado
*! pystacked v0.7.6
*! last edited: 3jan2025
*! authors: aa/ms

. 
. // Step 1: Specify the model.
. // We use the partially-linear model with 10-fold cross-fitting and a single cross-fit split.
. ddml init partial, kfolds(10) reps(1)
warning - model m0 already exists
all existing model results and variables will
be dropped and model m0 will be re-initialized

. 
. // Step 2: Choose the learners.
. // We use unregularized OLS, cross-validated lasso and random forest.
. // We use the same learners for Y and X for expositional simplicity only.
. ddml E[Y|X]: pystacked $Y $X                                                            ///
>                                                                 || method(ols)                          ///
>                                                                 || method(lassocv)                      ///
>                                                                 || method(rf)                           ///
>                                                                 , type(reg)
Learner Y1_pystacked added successfully.

. 
. ddml E[D|X]: pystacked $D $X                                                            ///
>                                                                 || method(ols)                          ///
>                                                                 || method(lassocv)                      ///
>                                                                 || method(rf)                           ///
>                                                                 , type(reg)
Learner D1_pystacked added successfully.

. 
. // Check that the learners have been entered correctly.
. ddml desc, learners

Model:                  partial, crossfit folds k=10, resamples r=1
Mata global (mname):    m0
Dependent variable (Y): A198new
 A198new learners:      Y1_pystacked
D equations (1):        sd_EE
 sd_EE learners:        D1_pystacked

Y learners (detail):
 Learner:     Y1_pystacked
              est cmd: pystacked A198new v104_ee settlement_ee polhierarchies_ee loggdp || method(ols) || method(lassocv) || method(rf) , type(reg)
D learners (detail):
 Learner:     D1_pystacked
              est cmd: pystacked sd_EE v104_ee settlement_ee polhierarchies_ee loggdp || method(ols) || method(lassocv) || method(rf) , type(reg)

. 
. // Step 3: Cross-fit.
. // We use short-stacking to combine the conditional expectations
. // of the different learners.
. // The default behavior of ddml+pystacked is to do standard stacking,
. // which is computationally intensive; short-stacking is faster.
. // To suppress standard stacking, we use the nostdstack option.
. ddml crossfit, shortstack nostdstack
Cross-fitting E[y|X] equation: A198new
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking
Cross-fitting E[D|X] equation: sd_EE
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking

. 
. // Step 4: Estimation
. // After Step 3, we have 4 sets of residualized Y and 4 of residualized D,
. // based on 4 sets of estimated conditional expectations:
. // one for each of the 3 learners plus one stacked (ensemble) estimate.
. // By default, ddml reports the short-stacked results:
. // the residualized dependent variable Y is regressed on the residualized
. // causal variable of interest D using OLS.
. // Note that if you execute the code your your results will vary
. // (usually only slightly) because the seed hasn't been set.
. ddml estimate, robust


Model:                  partial, crossfit folds k=10, resamples r=1
Mata global (mname):    m0
Dependent variable (Y): A198new
 A198new learners:      Y1_pystacked
D equations (1):        sd_EE
 sd_EE learners:        D1_pystacked

DDML estimation results:
spec  r     Y learner     D learner         b        SE 
  ss  1  [shortstack]          [ss]    -2.164    (0.741)

Shortstack DDML model
y-E[y|X]  = y-Y_A198new_ss_1                       Number of obs   =        74
D-E[D|X]  = D-D_sd_EE_ss_1 
------------------------------------------------------------------------------
             |               Robust
     A198new | Coefficient  std. err.      z    P>|z|     [95% conf. interval]
-------------+----------------------------------------------------------------
       sd_EE |   -2.16408   .7413585    -2.92   0.004    -3.617116    -.711044
       _cons |   .0000172    .052501     0.00   1.000     -.102883    .1029173
------------------------------------------------------------------------------
Stacking final estimator: nnls1


. 
. // Display the short-stack weights.
. ddml extract, show(ssweights)

short-stacked weights across resamples for A198new
final stacking estimator: nnls1
             learner  mean_weight        rep_1
    ols            1    2.176e-19    2.176e-19
lassocv            2    .48057557    .48057557
     rf            3    .51942443    .51942443

short-stacked weights across resamples for sd_EE
final stacking estimator: nnls1
             learner  mean_weight        rep_1
    ols            1            0            0
lassocv            2    .41567395    .41567395
     rf            3    .58432605    .58432605

. 
. // ddml creates conditional expectations for each learner.
. // With pystacked, the learners are numbered.
. // The final "_1" identifies the resample (the single cross-fit split).
. // Create residualized Y and D for each learner.
. cap drop Y_L1_r Y_L2_r Y_L3_r Y_ss_r D_L1_r D_L2_r D_L3_r D_ss_r

. gen Y_L1_r = $Y-Y1_pystacked_L1_1

. gen Y_L2_r = $Y-Y1_pystacked_L2_1

. gen Y_L3_r = $Y-Y1_pystacked_L3_1

. gen Y_ss_r = $Y-Y_A198new_ss_1

. gen D_L1_r = $D-D1_pystacked_L1_1

. gen D_L2_r = $D-D1_pystacked_L2_1

. gen D_L3_r = $D-D1_pystacked_L3_1

. gen D_ss_r = $D-D_sd_EE_ss_1

. 
. // 4 specification and scatterplots
. // Y and D learners = OLS
. // We suppress the output for the individual OLS regressions to save space.
. qui reg Y_L1_r D_L1_r, robust

. est store ddml_L1_L1, title("OLS learners")

. cap drop hat_1_1

. predict hat_1_1
(option xb assumed; fitted values)

. twoway (scatter Y_L1_r D_L1_r, msymbol(o) mlabel(isocode))                      ///
>         (line hat_1_1 D_L1_r, sort lwidth(thick)),                                              ///
>         legend(off)                                                                                                             ///
>         xlabel(-0.3 -0.2 -0.1 0 0.1 0.2 0.3, nogrid)                                    ///
>         ylabel(-2 -1 0 1 2, nogrid)                                                                             ///
>         title("Y and D: Unregularized OLS")                                                             ///
>         ytitle("Importance of tradition")                                                               ///
>         xtitle("Climatic instability")                                                                  ///
>         name(OLS, replace)

. // Y and D learners = CV Lasso
. qui reg Y_L2_r D_L2_r, robust

. est store ddml_L2_L2, title("Lasso learners")

. cap drop hat_2_2

. predict hat_2_2
(option xb assumed; fitted values)

. twoway (scatter Y_L2_r D_L2_r, msymbol(o) mlabel(isocode))                      ///
>         (line hat_2_2 D_L2_r, sort lwidth(thick)),                                              ///
>         legend(off)                                                                                                             ///
>         xlabel(-0.3 -0.2 -0.1 0 0.1 0.2 0.3, nogrid)                                    ///
>         ylabel(-2 -1 0 1 2, nogrid)                                                                             ///
>         title("Y and D: Lasso")                                                                                 ///
>         ytitle("Importance of tradition")                                                               ///
>         xtitle("Climatic instability")                                                                  ///
>         name(Lasso, replace)

. // Y and D learners = Random Forest
. qui reg Y_L3_r D_L3_r, robust

. est store ddml_L3_L3, title("RF learners")

. cap drop hat_3_3

. predict hat_3_3
(option xb assumed; fitted values)

. twoway (scatter Y_L3_r D_L3_r, msymbol(o) mlabel(isocode))                      ///
>         (line hat_3_3 D_L3_r, sort lwidth(thick)),                                              ///
>         legend(off)                                                                                                             ///
>         xlabel(-0.3 -0.2 -0.1 0 0.1 0.2 0.3, nogrid)                                    ///
>         ylabel(-2 -1 0 1 2, nogrid)                                                                             ///
>         title("Y and D: Random Forest")                                                                 ///
>         ytitle("Importance of tradition")                                                               ///
>         xtitle("Climatic instability")                                                                  ///
>         name(RF, replace)

. // Y and D learners = Short-stacked (ensemble of OLS, Lasso, RF)
. qui reg Y_ss_r D_ss_r, robust

. est store ddml_SS_SS, title("SS learners")

. cap drop hat_ss_ss

. predict hat_ss_ss
(option xb assumed; fitted values)

. twoway (scatter Y_ss_r D_ss_r, msymbol(o) mlabel(isocode))                      ///
>         (line hat_ss_ss D_ss_r, sort lwidth(thick)),                                    ///
>         legend(off)                                                                                                             ///
>         xlabel(-0.3 -0.2 -0.1 0 0.1 0.2 0.3, nogrid)                                    ///
>         ylabel(-2 -1 0 1 2, nogrid)                                                                             ///
>         title("Y and D: Short-stacked")                                                                 ///
>         ytitle("Importance of tradition")                                                               ///
>         xtitle("Climatic instability")                                                                  ///
>         name(SS, replace)

. 
. // Compare DML estimations for learner combinations:
. // nb: Code uses estout by Ben Jann; install from SSC Archives.
. estout ddml_L1_L1 ddml_L2_L2 ddml_L3_L3 ddml_SS_SS,                                     ///
>         label modelwidth(15) collabels(none)                                                    ///
>         rename (D_L1_r D_r D_L2_r D_r D_L3_r D_r D_ss_r D_r)                    ///
>         cells(b(fmt(3)) se(par fmt(3)))                                                                 ///
>         varlabels(_cons Constant)

------------------------------------------------------------------------------------
                        OLS learners  Lasso learners     RF learners     SS learners
------------------------------------------------------------------------------------
D_r                           -1.799          -1.839          -2.147          -2.164
                             (0.665)         (0.668)         (0.680)         (0.741)
Constant                      -0.020          -0.011           0.010           0.000
                             (0.055)         (0.055)         (0.054)         (0.053)
------------------------------------------------------------------------------------

. 
. // Compare via scatterplot:
. graph combine OLS Lasso RF SS

. graph export "images/GN_4learners.png", replace
file /Users/kahrens/MyProjects/DMLGuide.github.io/assets/images/GN_4learners.png saved as PNG format

. 
. // "Final" results:
. // 1. Set seed for replicability.
. // 2. Report all 3 stacking methods.
. // 3. 11 separate cross-fit splits and median aggregation.
. 
. // The choice of 11 separate cross-fit splits is so that the median is well-defined.
. 
. // Set the seed.
. set seed 42

. 
. // Step 1: Specify the model.
. // Use 11 separate resample splits.
. ddml init partial, kfolds(10) reps(11)
warning - model m0 already exists
all existing model results and variables will
be dropped and model m0 will be re-initialized

. 
. // Step 2: Choose the learners.
. // Conditional expections are defined as before.
. ddml E[Y|X]: pystacked $Y $X                                                            ///
>                                                                 || method(ols)                          ///
>                                                                 || method(lassocv)                      ///
>                                                                 || method(rf)                           ///
>                                                                 , type(reg)
Learner Y1_pystacked added successfully.

. 
. ddml E[D|X]: pystacked $D $X                                                            ///
>                                                                 || method(ols)                          ///
>                                                                 || method(lassocv)                      ///
>                                                                 || method(rf)                           ///
>                                                                 , type(reg)
Learner D1_pystacked added successfully.

. 
. // Step 3: Cross-fit.
. // Standard stacking is the default behavior of ddml+pystacked.
. // Pooled-stacking is requested separately.
. ddml crossfit, shortstack nostdstack
Cross-fitting E[y|X] equation: A198new
Resample 1...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking
Resample 2...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking
Resample 3...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking
Resample 4...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking
Resample 5...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking
Resample 6...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking
Resample 7...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking
Resample 8...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking
Resample 9...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking
Resample 10...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking
Resample 11...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking
Cross-fitting E[D|X] equation: sd_EE
Resample 1...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking
Resample 2...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking
Resample 3...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking
Resample 4...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking
Resample 5...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking
Resample 6...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking
Resample 7...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking
Resample 8...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking
Resample 9...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking
Resample 10...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking
Resample 11...
Cross-fitting fold 1 2 3 4 5 6 7 8 9 10 ...completed cross-fitting...completed short-stacking

. 
. // Step 4: Estimation.
. // Median-aggregated short-stacked results are reported by default.
. // We also ask for the standard and pooled-stacking results.
. ddml estimate, robust


Model:                  partial, crossfit folds k=10, resamples r=11
Mata global (mname):    m0
Dependent variable (Y): A198new
 A198new learners:      Y1_pystacked
D equations (1):        sd_EE
 sd_EE learners:        D1_pystacked

DDML estimation results:
spec  r     Y learner     D learner         b        SE 
  ss  1  [shortstack]          [ss]    -2.326    (0.802)
  ss  2  [shortstack]          [ss]    -1.907    (0.769)
  ss  3  [shortstack]          [ss]    -2.141    (0.723)
  ss  4  [shortstack]          [ss]    -2.058    (0.790)
  ss  5  [shortstack]          [ss]    -2.265    (0.797)
  ss  6  [shortstack]          [ss]    -2.108    (0.764)
  ss  7  [shortstack]          [ss]    -2.252    (0.735)
  ss  8  [shortstack]          [ss]    -1.892    (0.733)
  ss  9  [shortstack]          [ss]    -1.941    (0.749)
  ss 10  [shortstack]          [ss]    -2.231    (0.735)
  ss 11  [shortstack]          [ss]    -1.911    (0.763)

Mean/med    Y learner     D learner         b        SE 
  ss mn  [shortstack]          [ss]    -2.094    (0.774)
  ss md  [shortstack]          [ss]    -2.108    (0.767)

Shortstack DDML model (median over 11 resamples)
y-E[y|X]  = y-Y_A198new_ss                         Number of obs   =        74
D-E[D|X]  = D-D_sd_EE_ss 
------------------------------------------------------------------------------
             |               Robust
     A198new | Coefficient  std. err.      z    P>|z|     [95% conf. interval]
-------------+----------------------------------------------------------------
       sd_EE |  -2.108401   .7674987    -2.75   0.006    -3.612671   -.6041313
------------------------------------------------------------------------------
Stacking final estimator: nnls1

Summary over 11 resamples:
       D eqn      mean       min       p25       p50       p75       max
       sd_EE     -2.0937   -2.3256   -2.2521   -2.1084   -1.9105   -1.8915

. 
. // Display the stacking weights.
. // As before, unregularized OLS gets a low weight for both Y and D,
. // and the random forest learner gets a substantial weight in both.
. ddml extract, show(ssweights)

short-stacked weights across resamples for A198new
final stacking estimator: nnls1
             learner  mean_weight        rep_1        rep_2        rep_3        rep_4        rep_5        rep_6        rep_7        rep_8        rep_9       rep_10       rep_11
    ols            1     .2780639    .25021548    .44603695    .15314268    .68821837            0    8.412e-18    .11566583    .08583231    .34503221    .49644733    .47811173
lassocv            2    .15544152    1.031e-18    .11247891     .3152944    1.086e-19    .49763273    .31023968     .1796082    .28357956            0    1.952e-18    .01102318
     rf            3    .56649459    .74978452    .44148413    .53156292    .31178163    .50236727    .68976032    .70472597    .63058813    .65496779    .50355267    .51086509

short-stacked weights across resamples for sd_EE
final stacking estimator: nnls1
             learner  mean_weight        rep_1        rep_2        rep_3        rep_4        rep_5        rep_6        rep_7        rep_8        rep_9       rep_10       rep_11
    ols            1    1.490e-18            0            0    3.209e-18            0    6.072e-18    3.326e-18    1.502e-18            0            0    2.277e-18            0
lassocv            2    .33170229    .37136168    .34520346    .32768056    .27627495     .3283087    .38245886    .40774365    .37568256    .26729688    .30843848    .25827545
     rf            3    .66829771    .62863832    .65479654    .67231944    .72372505     .6716913    .61754114    .59225635    .62431744    .73270312    .69156152    .74172455

. 
. ********************************************************************************
. 
. cap log close
